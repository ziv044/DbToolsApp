# Story 4.12: Alert Evaluation Engine

## Status: Ready for Review

## Story

**As the** system,
**I want** to evaluate alert rules against collected metrics,
**so that** alerts are triggered when thresholds are exceeded.

## Acceptance Criteria

| # | Criteria |
|---|----------|
| 1 | Alert evaluator runs after each metric collection cycle |
| 2 | Compares latest metrics against all enabled alert rules |
| 3 | Creates alert record when threshold breached |
| 4 | Does not duplicate alerts (one active alert per rule per server) |
| 5 | Resolves alert when metric returns to normal for 2 consecutive checks |
| 6 | Updates server health status based on active alerts |
| 7 | Logs alert state changes to activity log |
| 8 | Supports evaluation of aggregate metrics (avg over 5 min) |
| 9 | Alert evaluation completes within 1 second per tenant |

## Tasks / Subtasks

- [x] Task 1: Create AlertEvaluator service (AC: 1, 2)
- [x] Task 2: Implement threshold checking (AC: 3, 4)
- [x] Task 3: Implement auto-resolve logic (AC: 5)
- [x] Task 4: Update server health (AC: 6)
- [x] Task 5: Add activity logging (AC: 7)
- [x] Task 6: Add aggregate support (AC: 8)

## Dev Notes

### Alert Evaluator
```python
class AlertEvaluator:
    OPERATORS = {
        'gt': lambda v, t: v > t,
        'gte': lambda v, t: v >= t,
        'lt': lambda v, t: v < t,
        'lte': lambda v, t: v <= t,
        'eq': lambda v, t: v == t,
    }

    def evaluate(self, server, snapshot, rules):
        for rule in rules:
            value = snapshot.metrics.get(rule.metric_type)
            if value is None:
                continue

            check = self.OPERATORS[rule.operator]
            if check(value, rule.threshold):
                self.trigger_or_update_alert(server, rule)
            else:
                self.maybe_resolve_alert(server, rule)

    def trigger_or_update_alert(self, server, rule):
        existing = Alert.query.filter_by(
            server_id=server.id,
            rule_id=rule.id,
            status='active'
        ).first()
        if not existing:
            alert = Alert(server_id=server.id, rule_id=rule.id, status='active')
            db.session.add(alert)
```

## Change Log

| Date | Version | Description | Author |
|------|---------|-------------|--------|
| 2026-01-17 | 0.1 | Initial story creation | PM (John) |

---

## Dev Agent Record

### Agent Model Used
Claude Opus 4.5 (claude-opus-4-5-20251101)

### Debug Log References
- No issues encountered during development

### Completion Notes List
- Created `backend/app/services/alert_evaluator.py`:
  - `AlertEvaluator` class with full alert evaluation logic:
    - `evaluate_server()` - evaluates all enabled rules against a server's metrics
    - `evaluate_all_servers()` - batch evaluation for all servers with recent snapshots
    - `evaluate_with_aggregates()` - evaluates using averaged metrics over a 5-minute window
    - `get_server_health_status()` - returns 'healthy', 'warning', or 'critical' based on active alerts
  - `RESOLVE_THRESHOLD = 2` - consecutive normal readings required for auto-resolve
  - Uses `AlertRule.evaluate()` method for threshold checking (supports gt, gte, lt, lte, eq operators)
  - Prevents duplicate alerts (one active alert per rule per server)
  - Auto-resolves alerts after 2 consecutive normal checks
  - Logs all alert state changes to ActivityLog
  - `run_alert_evaluation()` helper function for scheduler integration
- Added `ActivityLog` model to `backend/app/models/tenant.py`:
  - Action types: alert_triggered, alert_resolved, alert_acknowledged, job_executed, job_failed, policy_deployed, server_online, server_offline
  - Entity types: alert, job, policy, server
  - Fields: id, action, entity_type, entity_id, details (JSONB), created_at
  - Indexes on created_at and (entity_type, entity_id)
- Activity logging integrated into AlertEvaluator for alert_triggered and alert_resolved events
- All 72 backend tests pass

### File List
- backend/app/services/alert_evaluator.py (new)
- backend/app/models/tenant.py (modified - added ActivityLog model)

---

## QA Results
